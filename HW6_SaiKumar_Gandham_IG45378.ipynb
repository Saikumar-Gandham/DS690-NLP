{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2cece2",
   "metadata": {},
   "source": [
    "### Name : Sai Kumar Gandham\n",
    "### Student ID: IG45378"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43cf0e",
   "metadata": {},
   "source": [
    "Homework:\n",
    "\n",
    "1) Using Gensim, train a doc2vec model on the Brown Corpus. Try to classify documents from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ab38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import brown\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to read corpus from Brown Corpus\n",
    "def read_corpus(data, tokens_only=False):\n",
    "    for i, line in enumerate(data):\n",
    "        if tokens_only:\n",
    "            yield line\n",
    "        else:\n",
    "            # Join tokenized sentences into a single string\n",
    "            doc = ' '.join(line)\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(doc), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c199e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load Brown Corpus:\n",
    "brown_corpus = brown.sents(categories=brown.categories())\n",
    "\n",
    "# Prepare train and test corpus\n",
    "train_corpus = list(read_corpus(brown_corpus))\n",
    "test_corpus = list(read_corpus(brown_corpus, tokens_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9731484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n"
     ]
    }
   ],
   "source": [
    "#Train the Doc2Vec model\n",
    "max_epochs = 20\n",
    "vec_size = 100\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('Iteration {0}'.format(epoch))\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    # Decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # Fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49a03194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: adventure, Number of Documents: 4637\n",
      "Category: belles_lettres, Number of Documents: 7209\n",
      "Category: editorial, Number of Documents: 2997\n",
      "Category: fiction, Number of Documents: 4249\n",
      "Category: government, Number of Documents: 3032\n",
      "Category: hobbies, Number of Documents: 4193\n",
      "Category: humor, Number of Documents: 1053\n",
      "Category: learned, Number of Documents: 7734\n",
      "Category: lore, Number of Documents: 4881\n",
      "Category: mystery, Number of Documents: 3886\n",
      "Category: news, Number of Documents: 4623\n",
      "Category: religion, Number of Documents: 1716\n",
      "Category: reviews, Number of Documents: 1751\n",
      "Category: romance, Number of Documents: 4431\n",
      "Category: science_fiction, Number of Documents: 948\n"
     ]
    }
   ],
   "source": [
    "# Classify documents from each category\n",
    "category_docs = defaultdict(list)\n",
    "\n",
    "for cat in brown.categories():\n",
    "    for doc in brown.sents(categories=cat):\n",
    "        inferred_vector = model.infer_vector(doc)\n",
    "        category_docs[cat].append(inferred_vector)\n",
    "\n",
    "# Displaying the number of documents per category\n",
    "for category, vectors in category_docs.items():\n",
    "    print(f\"Category: {category}, Number of Documents: {len(vectors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dfe055",
   "metadata": {},
   "source": [
    "\n",
    "2) Use the stop word removal code from earlier on the 20 user groups:\n",
    "\n",
    "How does that effect the word mover distance of documents? Pick 6 documents to compare (make sure to use the same splits so they are the same documents).\n",
    "\n",
    "How does it effect the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ef7ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load 20 newsgroups dataset\n",
    "twenty_users = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08560141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define stop words\n",
    "stop_words = set(gensim.parsing.preprocessing.STOPWORDS)\n",
    "\n",
    "# Function to remove stop words and tokenize\n",
    "def preprocess(text, remove_stopwords=True):\n",
    "    if remove_stopwords:\n",
    "        return ' '.join([word for word in gensim.utils.simple_preprocess(text) if word not in stop_words])\n",
    "    else:\n",
    "        return ' '.join(gensim.utils.simple_preprocess(text))\n",
    "\n",
    "# Preprocess documents with stop words removed\n",
    "preprocessed_docs_stop = [preprocess(doc) for doc in twenty_users.data]\n",
    "preprocessed_docs_nostop = [preprocess(doc, remove_stopwords=False) for doc in twenty_users.data]\n",
    "\n",
    "# Vectorize preprocessed documents (with stop words removed)\n",
    "vectorizer_stop = TfidfVectorizer()\n",
    "X_stop = vectorizer_stop.fit_transform(preprocessed_docs_stop)\n",
    "\n",
    "# Vectorize preprocessed documents (without stop words removed)\n",
    "vectorizer_nostop = TfidfVectorizer()\n",
    "X_nostop = vectorizer_nostop.fit_transform(preprocessed_docs_nostop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7782c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression classifier with stop words removed\n",
    "trainX_stop, testX_stop, trainY_stop, testY_stop = train_test_split(X_stop, twenty_users.target)\n",
    "lr_stopwords = LogisticRegression(max_iter=500)\n",
    "lr_stopwords.fit(trainX_stop, trainY_stop)\n",
    "accuracy_stopwords = accuracy_score(testY_stop, lr_stopwords.predict(testX_stop))\n",
    "\n",
    "# Train logistic regression classifier without stop words removed\n",
    "trainX_nostop, testX_nostop, trainY_nostop, testY_nostop = train_test_split(X_nostop, twenty_users.target)\n",
    "lr_nostopwords = LogisticRegression(max_iter=500)\n",
    "lr_nostopwords.fit(trainX_nostop, trainY_nostop)\n",
    "accuracy_nostopwords = accuracy_score(testY_nostop, lr_nostopwords.predict(testX_nostop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37ac0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 6 documents to compare\n",
    "docs_indices = [10, 20, 30, 40, 50, 60]\n",
    "docs_to_compare = [twenty_users.data[i] for i in docs_indices]\n",
    "preprocessed_docs_to_compare = [preprocess(doc) for doc in docs_to_compare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa508c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effects of stop words on Word Mover's Distance (WMD):\n",
      "Mean WMD without stop words: inf\n",
      "Mean WMD with stop words removed: inf\n",
      "\n",
      "Effect of stop words on logistic regression classifier accuracy:\n",
      "Accuracy with stop words removed: 0.8992576882290562\n",
      "Accuracy without stop words removed: 0.8907741251325557\n"
     ]
    }
   ],
   "source": [
    "# Calculate Word Movers Distance  without stop words\n",
    "wmd_without_stopwords = []\n",
    "for doc in preprocessed_docs_to_compare:\n",
    "    wmd = model.wv.wmdistance(doc, preprocessed_docs_to_compare[0])  # Compare with the first document\n",
    "    wmd_without_stopwords.append(wmd)\n",
    "\n",
    "# Calculate Word Movers Distance with stop words removed\n",
    "wmd_with_stopwords = []\n",
    "for doc in docs_to_compare:\n",
    "    wmd = model.wv.wmdistance(preprocess(doc), preprocessed_docs_to_compare[0])\n",
    "    wmd_with_stopwords.append(wmd)\n",
    "\n",
    "print(\"Effects of stop words on Word Mover's Distance (WMD):\")\n",
    "print(\"Mean WMD without stop words:\", np.mean(wmd_without_stopwords))\n",
    "print(\"Mean WMD with stop words removed:\", np.mean(wmd_with_stopwords))\n",
    "print()\n",
    "\n",
    "print(\"Effect of stop words on logistic regression classifier accuracy:\")\n",
    "print(\"Accuracy with stop words removed:\", accuracy_stopwords)\n",
    "print(\"Accuracy without stop words removed:\", accuracy_nostopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e9968",
   "metadata": {},
   "source": [
    "The \"inf\" values for Word Movers Distance mean that for some document pairs, the distance between them is so large that it's considered infinite. . \n",
    "\n",
    "As for the logistic regression classifier, when we remove stop words, it slightly boosts the accuracy. With stop words removed, the classifier gets around 90% of the predictions correct, compared to about 89% when we keep stop words. This shows that taking out the less important words helps the classifier focus better on the main ideas in the text, leading to a bit better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a5517",
   "metadata": {},
   "source": [
    "#### Just trying to observe Some different outputs and doing further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312a87f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed document with stop words removed:\n",
      "lerxst wam umd edu thing subject car nntp posting host rac wam umd edu organization university maryland college park lines wondering enlighten car saw day door sports car looked late early called bricklin doors small addition bumper separate rest body know tellme model engine specs years production car history info funky looking car mail thanks il brought neighborhood lerxst\n",
      "\n",
      "Preprocessed document without stop words removed:\n",
      "from lerxst wam umd edu where my thing subject what car is this nntp posting host rac wam umd edu organization university of maryland college park lines was wondering if anyone out there could enlighten me on this car saw the other day it was door sports car looked to be from the late early it was called bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all know if anyone can tellme model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please mail thanks il brought to you by your neighborhood lerxst\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Load 20 newsgroups dataset\n",
    "twenty_users = fetch_20newsgroups()\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(gensim.parsing.preprocessing.STOPWORDS)\n",
    "\n",
    "# Function to remove stop words and tokenize\n",
    "def preprocess(text, remove_stopwords=True):\n",
    "    if remove_stopwords:\n",
    "        return ' '.join([word for word in gensim.utils.simple_preprocess(text) if word not in stop_words])\n",
    "    else:\n",
    "        return ' '.join(gensim.utils.simple_preprocess(text))\n",
    "\n",
    "# Preprocess documents with stop words removed\n",
    "preprocessed_docs_stop = [preprocess(doc) for doc in twenty_users.data]\n",
    "preprocessed_docs_nostop = [preprocess(doc, remove_stopwords=False) for doc in twenty_users.data]\n",
    "\n",
    "# Inspect a sample of preprocessed documents\n",
    "print(\"Preprocessed document with stop words removed:\")\n",
    "print(preprocessed_docs_stop[0])\n",
    "print(\"\\nPreprocessed document without stop words removed:\")\n",
    "print(preprocessed_docs_nostop[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35d8fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix (with stop words removed):\n",
      "[[1.00000000e+00 1.55563277e-02 3.31727749e-02 ... 3.48750542e-03\n",
      "  7.75473924e-03 2.00315282e-02]\n",
      " [1.55563277e-02 1.00000000e+00 2.79741207e-02 ... 5.66351706e-02\n",
      "  5.81717547e-02 1.92273151e-02]\n",
      " [3.31727749e-02 2.79741207e-02 1.00000000e+00 ... 1.64039422e-03\n",
      "  1.11668738e-02 1.39015404e-02]\n",
      " ...\n",
      " [3.48750542e-03 5.66351706e-02 1.64039422e-03 ... 1.00000000e+00\n",
      "  2.60415660e-03 7.69593259e-04]\n",
      " [7.75473924e-03 5.81717547e-02 1.11668738e-02 ... 2.60415660e-03\n",
      "  1.00000000e+00 5.77604860e-03]\n",
      " [2.00315282e-02 1.92273151e-02 1.39015404e-02 ... 7.69593259e-04\n",
      "  5.77604860e-03 1.00000000e+00]]\n",
      "\n",
      "Classifier Performance (with stop words removed):\n",
      "Accuracy: 0.9042064333686816\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.93      0.94      0.94       122\n",
      "           comp.graphics       0.71      0.85      0.78       141\n",
      " comp.os.ms-windows.misc       0.84      0.88      0.86       148\n",
      "comp.sys.ibm.pc.hardware       0.85      0.78      0.81       156\n",
      "   comp.sys.mac.hardware       0.88      0.84      0.86       155\n",
      "          comp.windows.x       0.90      0.93      0.92       137\n",
      "            misc.forsale       0.84      0.88      0.86       143\n",
      "               rec.autos       0.89      0.91      0.90       140\n",
      "         rec.motorcycles       0.98      0.94      0.96       148\n",
      "      rec.sport.baseball       0.93      0.95      0.94       146\n",
      "        rec.sport.hockey       0.95      0.95      0.95       162\n",
      "               sci.crypt       1.00      0.93      0.96       169\n",
      "         sci.electronics       0.88      0.88      0.88       161\n",
      "                 sci.med       0.95      0.89      0.92       142\n",
      "               sci.space       0.95      0.98      0.96       144\n",
      "  soc.religion.christian       0.90      0.95      0.92       146\n",
      "      talk.politics.guns       0.95      0.93      0.94       134\n",
      "   talk.politics.mideast       0.95      1.00      0.97       125\n",
      "      talk.politics.misc       0.94      0.92      0.93       133\n",
      "      talk.religion.misc       0.94      0.66      0.78        77\n",
      "\n",
      "                accuracy                           0.90      2829\n",
      "               macro avg       0.91      0.90      0.90      2829\n",
      "            weighted avg       0.91      0.90      0.90      2829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize preprocessed documents\n",
    "vectorizer_stop = TfidfVectorizer()\n",
    "X_stop = vectorizer_stop.fit_transform(preprocessed_docs_stop)\n",
    "\n",
    "# Training the  logistic regression classifier with stop words removed\n",
    "trainX_stop, testX_stop, trainY_stop, testY_stop = train_test_split(X_stop, twenty_users.target)\n",
    "lr_stopwords = LogisticRegression(max_iter=500)\n",
    "lr_stopwords.fit(trainX_stop, trainY_stop)\n",
    "predictions_stop = lr_stopwords.predict(testX_stop)\n",
    "\n",
    "# Calculating cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculating cosine similarity matrix for documents with stop words removed\n",
    "cosine_sim_stop = cosine_similarity(X_stop)\n",
    "\n",
    "# Printing cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix (with stop words removed):\")\n",
    "print(cosine_sim_stop)\n",
    "\n",
    "# Analyzing classifier performance\n",
    "accuracy_stopwords = accuracy_score(testY_stop, predictions_stop)\n",
    "report_stopwords = classification_report(testY_stop, predictions_stop, target_names=twenty_users.target_names)\n",
    "\n",
    "print(\"\\nClassifier Performance (with stop words removed):\")\n",
    "print(\"Accuracy:\", accuracy_stopwords)\n",
    "print(\"Classification Report:\\n\", report_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246b673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
