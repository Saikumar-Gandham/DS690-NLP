{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9fc812",
   "metadata": {},
   "source": [
    "### Name: Sai Kumar Gandham\n",
    "### Student ID : IG45378"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f7e0e",
   "metadata": {},
   "source": [
    "### Chapter-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a868326",
   "metadata": {},
   "source": [
    "#### 6. Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "a. [a-zA-Z]+\n",
    "\n",
    "b. [A-Z][a-z]*\n",
    "\n",
    "c. p[aeiou]{,2}t\n",
    "\n",
    "d. \\d+(\\.\\d+)?\n",
    "\n",
    "e. ([^aeiou][aeiou][^aeiou])*\n",
    "\n",
    "f. \\w+|[^\\w\\s]+\n",
    "\n",
    "**Test your answers using ** nltk.re_show()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f428ce8",
   "metadata": {},
   "source": [
    "\n",
    "a. [a-zA-Z]+: This pattern matches words containing only letters. It could be one or more letters in a row, either uppercase or lowercase.\n",
    "\n",
    "b. [A-Z][a-z]*: This pattern matches words starting with a capital letter, followed by zero or more lowercase letters.\n",
    "\n",
    "c. p[aeiou]{,2}t: This pattern matches words that start with the letter 'p', followed by up to two vowels (a, e, i, o, or u), and ending with the letter 't'.\n",
    "\n",
    "d. \\d+(\\.\\d+)?: This pattern matches numbers, including decimals. It could be one or more digits, optionally followed by a decimal point and more digits.\n",
    "\n",
    "e. ([^aeiou][aeiou][^aeiou])*: This pattern matches sequences where a consonant is followed by a vowel, then followed by another consonant. It can repeat zero or more times.\n",
    "\n",
    "f. \\w+|[^\\w\\s]+: This pattern matches either words (consisting of letters, digits, or underscores) or sequences of non-word characters (excluding whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef1b6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regular Expression: \\b[a-zA-Z]+\\b\n",
      "{apple}\n",
      "{Banana}\n",
      "{pot}\n",
      "{potato}\n",
      "123\n",
      "12.34\n",
      "{bag}\n",
      "{beet}\n",
      "{boot}\n",
      "{xyx}\n",
      "word1\n",
      "!@#$\n",
      "\n",
      "Regular Expression: \\b[A-Z][a-z]*\\b\n",
      "apple\n",
      "{Banana}\n",
      "pot\n",
      "potato\n",
      "123\n",
      "12.34\n",
      "bag\n",
      "beet\n",
      "boot\n",
      "xyx\n",
      "word1\n",
      "!@#$\n",
      "\n",
      "Regular Expression: \\bp[aeiou]{,2}t\\b\n",
      "apple\n",
      "Banana\n",
      "{pot}\n",
      "potato\n",
      "123\n",
      "12.34\n",
      "bag\n",
      "beet\n",
      "boot\n",
      "xyx\n",
      "word1\n",
      "!@#$\n",
      "\n",
      "Regular Expression: \\d+(\\.\\d+)?\n",
      "apple\n",
      "Banana\n",
      "pot\n",
      "potato\n",
      "{123}\n",
      "{12.34}\n",
      "bag\n",
      "beet\n",
      "boot\n",
      "xyx\n",
      "word{1}\n",
      "!@#$\n",
      "\n",
      "Regular Expression: [^aeiou][aeiou][^aeiou]\n",
      "apple\n",
      "{Ban}ana\n",
      "{pot}\n",
      "{pot}ato\n",
      "123\n",
      "12.34\n",
      "{bag}\n",
      "beet\n",
      "boot\n",
      "xyx\n",
      "{wor}d1\n",
      "!@#$\n",
      "\n",
      "Regular Expression: \\w+|[^\\w\\s]+\n",
      "{apple}\n",
      "{Banana}\n",
      "{pot}\n",
      "{potato}\n",
      "{123}\n",
      "{12}{.}{34}\n",
      "{bag}\n",
      "{beet}\n",
      "{boot}\n",
      "{xyx}\n",
      "{word1}\n",
      "{!@#$}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Test strings\n",
    "test_strings = [\n",
    "    \"apple\",\n",
    "    \"Banana\",\n",
    "    \"pot\",\n",
    "    \"potato\",\n",
    "    \"123\",\n",
    "    \"12.34\",\n",
    "    \"bag\",\n",
    "    \"beet\",\n",
    "    \"boot\",\n",
    "    \"xyx\",\n",
    "    \"word1\",\n",
    "    \"!@#$\",\n",
    "]\n",
    "\n",
    "# Regular expressions\n",
    "regex_patterns = [\n",
    "    r'\\b[a-zA-Z]+\\b',\n",
    "    r'\\b[A-Z][a-z]*\\b',\n",
    "    r'\\bp[aeiou]{,2}t\\b',\n",
    "    r'\\d+(\\.\\d+)?',\n",
    "    r'[^aeiou][aeiou][^aeiou]',\n",
    "    r'\\w+|[^\\w\\s]+',\n",
    "]\n",
    "\n",
    "# Testing regular expressions\n",
    "for pattern in regex_patterns:\n",
    "    print(\"\\nRegular Expression:\", pattern)\n",
    "    for string in test_strings:\n",
    "        nltk.re_show(pattern, string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1c2f9",
   "metadata": {},
   "source": [
    "#### 7.Write regular expressions to match the following classes of strings:\n",
    "\n",
    "a. A single determiner (assume that a, an, and the are the only determiners)\n",
    "\n",
    "b. An arithmetic expression using integers, addition, and multiplication, such as\n",
    "2*3+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d26cb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "\n",
    "r'\\b(a|an|the)\\b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "856173c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b\n",
    "r'\\d+(\\*\\d+|\\+\\d+)*'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42ccaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for re_a: ['a', 'the', 'an']\n",
      "Matches for re_b: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example string\n",
    "example_string = \"I saw a cat on the mat. The cat was chasing an apple.\"\n",
    "\n",
    "# Regular expressions\n",
    "re_a = r'\\b(a|an|the)\\b'\n",
    "re_b = r'\\d+(\\*\\d+|\\+\\d+)*'\n",
    "\n",
    "# Finding matches using regular expression re_a\n",
    "matches_a = re.findall(re_a, example_string)\n",
    "print(\"Matches for re_a:\", matches_a)\n",
    "\n",
    "# Finding matches using regular expression re_b\n",
    "matches_b = re.findall(re_b, example_string)\n",
    "print(\"Matches for re_b:\", matches_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6af9b",
   "metadata": {},
   "source": [
    "#### 21. Write a function ** unknown() ** that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using ** re.findall() **) and remove any items from this set that occur in the Words Corpus ** (nltk.corpus.words) **. Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9c612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from nltk.corpus import words\n",
    "\n",
    "def unknown(url):\n",
    "    # Here we are fetching HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Here we are extracting all substrings consisting of lowercase letters\n",
    "    lowercase_words = re.findall(r'\\b[a-z]+\\b', html_content.lower())\n",
    "    \n",
    "    # Remove words from the set that occur in the Words Corpus\n",
    "    english_words = set(words.words())\n",
    "    unknown_words = [word for word in lowercase_words if word not in english_words]\n",
    "    \n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52114f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words on the web page: ['doctype', 'html', 'html', 'charset', 'utf', 'http', 'equiv', 'html', 'charset', 'utf', 'viewport', 'css', 'ui', 'blinkmacsystemfont', 'segoe', 'ui', 'helvetica', 'neue', 'helvetica', 'arial', 'fdfdff', 'rgba', 'visited', 'max', 'examples', 'documents', 'coordination', 'asking', 'href', 'https', 'www', 'iana', 'org', 'domains', 'html']\n"
     ]
    }
   ],
   "source": [
    "#Example:\n",
    "url = \"https://example.com\"\n",
    "unknown_words = unknown(url)\n",
    "print(\"Unknown words on the web page:\", unknown_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd328bca",
   "metadata": {},
   "source": [
    "HTML-related Terms: The presence of terms like 'doctype', 'html', 'charset', and 'http' suggests that the webpage contains HTML markup or references to HTML-related concepts.\n",
    "\n",
    "Typography and Style: Terms such as 'helvetica', 'rgba', and 'blinkmacsystemfont' indicate the inclusion of styling instructions for text and elements, reflecting the webpage's design and layout.\n",
    "\n",
    "Common English Words with Variations: Some common English words like 'documents' and 'examples' are present, indicating that the webpage may contain regular English text alongside HTML or other content.\n",
    "\n",
    "Domain-specific Terms or URLs: Terms like 'https', 'www', and 'iana' may refer to domain-specific terms or URLs, suggesting links or references to external websites or resources related to the webpage's content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad854774",
   "metadata": {},
   "source": [
    "#### 30. Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer, and see if you ob- serve any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f57071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer normalized text: ['run', 'fli', 'cat', 'troubl', 'troubl', 'troubl', 'friendship', 'friend', 'studi', 'studi', 'meet', 'meet']\n",
      "Lancaster Stemmer normalized text: ['run', 'fli', 'cat', 'troubl', 'troubl', 'troubl', 'friend', 'friend', 'study', 'study', 'meet', 'meet']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "# Sample tokenized text\n",
    "tokenized_text = [\"running\", \"flies\", \"cats\", \"trouble\", \"troubling\", \"troubled\", \"friendship\", \"friends\", \"studies\", \"studying\", \"meeting\", \"meeting\"]\n",
    "\n",
    "# Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "porter_stems = [porter.stem(word) for word in tokenized_text]\n",
    "print(\"Porter Stemmer normalized text:\", porter_stems)\n",
    "\n",
    "# Lancaster Stemmer\n",
    "lancaster = LancasterStemmer()\n",
    "lancaster_stems = [lancaster.stem(word) for word in tokenized_text]\n",
    "print(\"Lancaster Stemmer normalized text:\", lancaster_stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ceecd",
   "metadata": {},
   "source": [
    "#### differences :\n",
    "\n",
    "The Porter Stemmer produces \"friendship\" as the stem because it tries to preserve the root word when possible, resulting in a longer stem.\n",
    "\n",
    "The Lancaster Stemmer produces \"friend\" as the stem because it applies more aggressive stemming rules, resulting in a shorter stem.\n",
    "\n",
    "\n",
    "The Porter Stemmer produces \"studi\" as the stem, which is a less common form in English but retains more of the original word.\n",
    "\n",
    "The Lancaster Stemmer produces \"study\" as the stem, which is a more common form and reflects the root of the word more clearly.\n",
    "\n",
    "These differences arise from the strategies employed by each stemmer algorithm. The Porter Stemmer generally aims for accuracy and preservation of the original word form, while the Lancaster Stemmer prioritizes simplicity and may sacrifice some accuracy for efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f410ab8",
   "metadata": {},
   "source": [
    "#### 39. Read the Wikipedia entry on Soundex. Implement this algorithm in Python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc797dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R163\n",
      "R163\n",
      "R150\n"
     ]
    }
   ],
   "source": [
    "def soundex(name):\n",
    "    # Step 1: Convert name to uppercase\n",
    "    name = name.upper()\n",
    "    \n",
    "    # Step 2: Replace consonants with digits, remove vowels and 'h', 'w'\n",
    "    encoding = name[0]\n",
    "    mapping = {\n",
    "        'BFPV': '1', 'CGJKQSXZ': '2', 'DT': '3', 'L': '4', 'MN': '5', 'R': '6'\n",
    "    }\n",
    "    for char in name[1:]:\n",
    "        for key in mapping:\n",
    "            if char in key:\n",
    "                code = mapping[key]\n",
    "                if code != encoding[-1]:  # Avoid duplicates\n",
    "                    encoding += code\n",
    "                break\n",
    "    else:\n",
    "        encoding += '0'  # Here we are adding placeholder if encoding is shorter than 4 characters\n",
    "    \n",
    "    # Step 3: Replace the repeated digits with a single digit\n",
    "    encoding = ''.join(char for i, char in enumerate(encoding) if i == 0 or char != encoding[i-1])\n",
    "    \n",
    "    # Step 4: Remove zeros and truncate to 4 characters\n",
    "    encoding = encoding.replace('0', '')\n",
    "    encoding = encoding[:4].ljust(4, '0')\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "# Test the function\n",
    "print(soundex(\"Robert\"))  \n",
    "print(soundex(\"Rupert\")) \n",
    "print(soundex(\"Rubin\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e544541",
   "metadata": {},
   "source": [
    "### Chapter-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353bc260",
   "metadata": {},
   "source": [
    "#### 14. Write a function novel10(text) that prints any word that appeared in the last 10% of a text that had not been encountered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e4de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some\n",
      "new\n",
      "words.\n"
     ]
    }
   ],
   "source": [
    "def novel10(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the index from where the last 10% of words start\n",
    "    last_10_percent_index = int(len(words) * 0.9)\n",
    "    \n",
    "    # Get the words from the last 10%\n",
    "    last_10_percent_words = words[last_10_percent_index:]\n",
    "    \n",
    "    # Keep track of encountered words\n",
    "    encountered_words = set()\n",
    "    \n",
    "    # Iterate through the last 10% of words\n",
    "    for word in last_10_percent_words:\n",
    "        # Check if the word hasn't been encountered earlier\n",
    "        if word not in encountered_words:\n",
    "            # Print the word\n",
    "            print(word)\n",
    "            # Add the word to encountered set\n",
    "            encountered_words.add(word)\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is a sample text. It contains some words that repeat. But in the last 10% of the text, we might encounter some new words.\"\n",
    "novel10(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc916c34",
   "metadata": {},
   "source": [
    "#### 20 . Write a function that takes a list of words (containing duplicates) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g., if the input list contained 10 instances of the word table and 9 instances of the word chair, then table would appear before chair in the output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54192fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sort_words_by_frequency(words):\n",
    "    # here we are counting the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Sorting the words based on their frequency in descending order\n",
    "    sorted_words = sorted(word_counts, key=lambda x: word_counts[x], reverse=True)\n",
    "    \n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9b97b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table', 'chair', 'sofa', 'bed', 'lamp', 'rug']\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "words = [\"table\", \"chair\", \"table\", \"sofa\", \"chair\", \"table\", \"bed\", \"chair\", \"lamp\", \"table\", \"chair\", \"rug\", \"table\"]\n",
    "sorted_unique_words = sort_words_by_frequency(words)\n",
    "print(sorted_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ee461",
   "metadata": {},
   "source": [
    "\n",
    "#### 25. Read about string edit distance and the Levenshtein Algorithm. Try the imple- mentation provided in nltk.edit_dist(). In what way is this using dynamic pro- gramming? Does it use the bottom-up or top-down approach? (See also http:// norvig.com/spell-correct.html.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f1375",
   "metadata": {},
   "source": [
    "The Levenshtein distance, also known as the edit distance, is a measure of the similarity between two strings by calculating the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\n",
    "\n",
    "The Levenshtein Algorithm is a dynamic programming algorithm used to calculate the Levenshtein distance between two strings efficiently. It uses a bottom-up approach to construct a matrix where each cell represents the edit distance between substrings of the two strings.\n",
    "\n",
    "The nltk.edit_dist() function implements the Levenshtein Algorithm and calculates the edit distance between two strings. It uses dynamic programming by building a matrix iteratively, starting from the smallest substrings and gradually expanding to the entire strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2abe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance between 'kitten' and 'sitting': 3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Example strings\n",
    "string1 = \"kitten\"\n",
    "string2 = \"sitting\"\n",
    "\n",
    "# Calculating the edit distance using nltk.edit_dist()\n",
    "edit_distance = nltk.edit_distance(string1, string2)\n",
    "\n",
    "print(\"Edit distance between '{}' and '{}': {}\".format(string1, string2, edit_distance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70579991",
   "metadata": {},
   "source": [
    "\n",
    "#### 26. The Catalan numbers arise in many applications of combinatorial mathematics, including the counting of parse trees (Section 8.6). The series can be defined as follows: C0 = 1, and Cn+1 = Σ0..n (CiCn-i).\n",
    "\n",
    "a. Write a recursive function to compute nth Catalan number Cn.\n",
    "\n",
    "b. Now write another function that does this computation using dynamic programming.\n",
    "\n",
    "c. Use the timeit module to compare the performance of these functions as n increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf019b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.\n",
    "def catalan_recursive(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        catalan = 0\n",
    "        for i in range(n):\n",
    "            catalan += catalan_recursive(i) * catalan_recursive(n - i - 1)\n",
    "        return catalan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea0e149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalan_recursive(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca86278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b.\n",
    "def catalan_dp(n):\n",
    "    catalan = [0] * (n + 1)\n",
    "    catalan[0] = 1\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(i):\n",
    "            catalan[i] += catalan[j] * catalan[i - j - 1]\n",
    "    \n",
    "    return catalan[n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609bc54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalan_dp(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5322cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 10\n",
      "Recursive: 8.781927542000005\n",
      "Dynamic Programming: 0.03702654099998881\n",
      "\n",
      "n = 11\n",
      "Recursive: 26.542942374999996\n",
      "Dynamic Programming: 0.01154195899999877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#c.\n",
    "import timeit\n",
    "\n",
    "# Testing the performance\n",
    "for n in range(10, 12):  #Here we are comparing performance for n from 10 to 11\n",
    "    print(f\"n = {n}\")\n",
    "    print(\"Recursive:\", timeit.timeit(f\"catalan_recursive({n})\", globals=globals(), number=1000))\n",
    "    print(\"Dynamic Programming:\", timeit.timeit(f\"catalan_dp({n})\", globals=globals(), number=1000))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fab10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
